{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Optimization\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal of optimization is to find either the maximum or minimum $x^*$ of a given function $f(x)$. For a local optimum (minimum/maximum) we know from calculus, that the derivative of the function must be zero $f'(x^*)=0$. In simple cases this allows us to find the optimum analytically, e.g.\n",
    "$$f(x) = x^2 \\qquad \\Longrightarrow \\qquad f'(x)=2x \\qquad \\Longrightarrow \\qquad x^*=0.$$\n",
    "However, in many cases this might not be possible anymore such that an optimum can only be found through computational methods.\n",
    "\n",
    "### Gradient Method\n",
    "Given a function of several variables $f(x) \\in \\mathbb{R}$ with $x \\in \\mathbb{R}^n$, i.e. $f(x_1,x_2,...,x_n)$, the gradient $\\nabla f(x)$ is understood as the derivative of the function. This also means that a local optimum $x^*$ satisfies $\\nabla f(x^*)$. It is defined as follows\n",
    "$$\\nabla f(x) = \\begin{pmatrix}\n",
    "\\partial_{x_1}f(x) \\\\\n",
    "\\vdots \\\\\n",
    "\\partial_{x_n}f(x)\n",
    "\\end{pmatrix}.$$\n",
    "Here, $\\partial_{x_i}f(x)$ denotes the partial derivative with respect to $x_i$.\n",
    "\n",
    "#### Example\n",
    "$$f(x) = \\cos(x_1) + x_2^2 \\qquad \\Longrightarrow \\qquad \\partial_{x_1} f(x) = -\\sin(x_1), \\qquad \\partial_{x_2} f(x) =2x_2.$$\n",
    "In this example the gradient will be\n",
    "$$\\nabla f(x) = \\begin{pmatrix}\n",
    " -\\sin(x_1) \\\\\n",
    "2x_2\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "\n",
    "In 2D such a function can be visualized using a contourplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = np.linspace(0,2*np.pi,100)\n",
    "x2 = np.linspace(-2,2,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "f = lambda x: np.cos(x[0]) + x[1]**2\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.contourf(X1, X2, f([X1, X2]), 50, cmap='jet', alpha = 0.3)\n",
    "plt.colorbar()\n",
    "plt.contour(X1, X2, f([X1, X2]), 50, colors='black', linewidths=1, alpha=0.3)\n",
    "\n",
    "# Choose some point\n",
    "point = np.array([5, 1.3])\n",
    "\n",
    "# Calculate gradient at that point\n",
    "gradient = np.array([-np.sin(point[0]), 2*point[1]])\n",
    "\n",
    "# Plot the negative gradient\n",
    "plt.quiver(*point, *-gradient, angles='xy', scale_units='xy', scale=1, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is a vector that always points in the direction of the largest slope. Therefore, following the negative gradient should lead to the minimum of a function. We can do this by starting with any point $x^{(0)}$ and update this point in every iteration such that\n",
    "$$x^{(k+1)} = x^{(k)} - \\alpha \\nabla f(x^{(k)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0,2*np.pi,100)\n",
    "x2 = np.linspace(-2,2,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.contourf(X1, X2, f([X1, X2]), 50, cmap='jet', alpha = 0.3)\n",
    "plt.colorbar()\n",
    "plt.contour(X1, X2, f([X1, X2]), 50, colors='black', linewidths=1, alpha=0.3)\n",
    "\n",
    "# Define gradient\n",
    "df = lambda x: np.array([-np.sin(x[0]), 2*x[1]])\n",
    "\n",
    "# Choose some starting point\n",
    "x0 = np.array([2, -1.7])\n",
    "\n",
    "# step size (in machine learning it is called learning rate)\n",
    "α = 0.3\n",
    "\n",
    "for k in range(10):\n",
    "    plt.quiver(*x0, *-α*df(x0), angles='xy', scale_units='xy', scale=1, color='k')\n",
    "    x0 = x0 - α*df(x0)\n",
    "    \n",
    "print('Function values =', f(x0))\n",
    "print('Gradient = ', df(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step size strategy (Armijo Rule)\n",
    "\n",
    "Using a fixed step size $\\alpha$ often leads to oscillations around the local minimum such that the true value will never be found. Adaptive step sizes allow us to avoid this problem and guarantee that the method converges towards the minimum. The Armijo rule goes like this\n",
    "\\begin{align*}\n",
    "\\text{if} \\qquad &f(x - \\alpha^{(j)} \\nabla f(x))  - f(x) > -c \\alpha^{(j)} \\nabla f(x) \\cdot \\nabla f(x)\\\\\n",
    "\\text{then} \\qquad &\\alpha^{(j+1)} = 0.5\\alpha^{(j)}.\n",
    "\\end{align*}\n",
    "Parameter $c$ is usually chosen to be small, e.g. $10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0,2*np.pi,100)\n",
    "x2 = np.linspace(-2,2,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.contourf(X1, X2, f([X1, X2]), 50, cmap='jet', alpha = 0.3)\n",
    "plt.colorbar()\n",
    "plt.contour(X1, X2, f([X1, X2]), 50, colors='black', linewidths=1, alpha=0.3)\n",
    "\n",
    "# Define gradient\n",
    "df = lambda x: np.array([-np.sin(x[0]), 2*x[1]])\n",
    "\n",
    "# Choose some starting point\n",
    "x0 = np.array([np.random.uniform(0,2*np.pi), np.random.uniform(-2,2)])\n",
    "\n",
    "for k in range(10):\n",
    "    α = 1\n",
    "    x = x0 - α*df(x0)\n",
    "    while f(x) - f(x0) > -1e-4*α*np.dot(f(x0), f(x0)):\n",
    "        α = α/2\n",
    "        x = x0 - α*df(x0)\n",
    "    plt.quiver(*x0, *-α*df(x0), angles='xy', scale_units='xy', scale=1, color='k')\n",
    "    x0 = x#x0 - α*df(x0)\n",
    "    \n",
    "print('Function values =', f(x0))\n",
    "print('Gradient = ', df(x0))\n",
    "print('Solution = ', x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete function (without plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_method(f, df, x0, tol=1e-8, maxit=20):\n",
    "    norm = 1\n",
    "    norm = np.dot(df(x0), df(x0))**(1/2)\n",
    "    k = 1\n",
    "    while norm > tol and k < maxit+1:\n",
    "        α = 1\n",
    "        x = x0 - α*df(x0)\n",
    "        while f(x) - f(x0) > -1e-4*α*norm**2:\n",
    "            α = α/2\n",
    "            x = x0 - α*df(x0)\n",
    "            \n",
    "        x0 = x\n",
    "        norm = np.dot(df(x0), df(x0))**(1/2)\n",
    "        print('Iteration = ' + str(k) + '  Norm = ' + str(norm))\n",
    "        k += 1\n",
    "            \n",
    "    return x0\n",
    "              \n",
    "x = gradient_method(f, df, np.array([0.8,-1.7]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order methods\n",
    "The gradient method is a first order optimization method, i.e. the error towards the optimum reduces linearly in every step. For faster convergence we want to make us of a higher order method.\n",
    "\n",
    "#### Newton's method\n",
    "Newton's method is a (locally) second order method that was originally not designed to minimize a function but instead solve equations of the form\n",
    "$$F(x) = 0, \\qquad \\text{with } F(x) \\in \\mathbb{R^n} \\text{ and } x \\in \\mathbb{R^n}.$$\n",
    "The derivative of $F$ will therefore be a matrix with $n$ rows and $n$ columns, i.e. $F'(x) \\in \\mathbb{R^{n\\times n}}$. It is usually referred to as the Jacobi matrix of $F$ and is defined like this\n",
    "$$F'(x) :=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{\\partial F_1}{\\partial x_1}(x)&\\cdots&\\dfrac{\\partial F_1}{\\partial x_n}(x)\\\\\n",
    "\\vdots&\\ddots &\\vdots\\\\\n",
    "\\dfrac{\\partial F_n}{\\partial x_1}(x)&\\cdots&\\dfrac{\\partial F_n}{\\partial x_n}(x)\n",
    "\\end{pmatrix}$$\n",
    "Similar to the gradient method, Newton's method follows a simple iteration with\n",
    "$$x^{(k+1)} = x^{(k)} - F'(x^{(k)})^{-1}F(x^{(k)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve, norm\n",
    "\n",
    "def Newton(F, dF, x0, tol=1e-4, maxit=20):\n",
    "    k = 0\n",
    "    res = 1\n",
    "    x0 = np.array(x0)\n",
    "    fnorm = norm(F(x0))\n",
    "    print('Iteration =', k, 'Residual =', res, ', Norm =', fnorm)\n",
    "    while k < maxit+1 and res > tol and fnorm > 1e-3*tol:\n",
    "        if x0.shape == ():\n",
    "            dx0 = -F(x0)/dF(x0)\n",
    "            res = abs(dx0/x0)\n",
    "        else:\n",
    "            dx0 = -solve(dF(x0), F(x0))\n",
    "            res = norm(dx0)/norm(x0)\n",
    "            \n",
    "        x = x0 + dx0\n",
    "        x0 = x\n",
    "        k += 1\n",
    "        fnorm = norm(F(x0))\n",
    "        print('Iteration =', k, 'Residual =', res, ', Norm =', fnorm)\n",
    "        \n",
    "    return x\n",
    "\n",
    "F = lambda x: np.array([np.sin(x[0]), np.cos(x[1])])\n",
    "dF = lambda x: np.array([[np.cos(x[0]), np.cos(x[1])],\n",
    "                        [-np.sin(x[0]), -np.sin(x[1])]])\n",
    "\n",
    "x = Newton(F, dF, np.array([3,1]), tol = 1e-10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application to optimization\n",
    "Newton's method cannot be directly applied to an optimization problem. However, we know that an optimum $x^*$ satisfies $\\nabla f(x^*) = 0$. Therefore, we define $F(x) := \\nabla f(x)$ and solve this equation instead. The derivative $F'(x)$ is usually called the Hessian matrix and is usually denoted by $H_f(x)$. It is basically the Jacobian matrix of the Gradient $\\nabla f$ and can be written as\n",
    "$$H_f(x) :=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_1}(x)&\\cdots&\\dfrac{\\partial^2 f}{\\partial x_1 \\partial x_n}(x)\\\\\n",
    "\\vdots&\\ddots &\\vdots\\\\\n",
    "\\dfrac{\\partial^2 f}{\\partial x_n \\partial x_1}(x)&\\cdots&\\dfrac{\\partial^2 f}{\\partial x_n \\partial x_n}(x).\n",
    "\\end{pmatrix}$$\n",
    "If the gradient $\\nabla f$ desrcibes first derivate of a function $f$, then the Hessian $H_f$ can be considered the second derivative.\n",
    "##### Remark: The higher the order of a method, the more derivatives of a function we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda x: np.cos(x[0]) + x[1]**2\n",
    "df = lambda x: np.array([-np.sin(x[0]), 2*x[1]])\n",
    "d2f = lambda x: np.array([[-np.cos(x[0]), 0],[0, 2]])\n",
    "\n",
    "x0 =np.array([3,1])\n",
    "x = Newton(df, d2f, x0, tol = 1e-10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final remarks\n",
    "We have seen examples of optimization in two dimensions, with the code being generalized to $n$ dimensions. However, depending on the exact problem formulation, the concept of the gradient and second derivatives usually have to be broadened even further. Popular fields are\n",
    "- constrained optimization\n",
    "- optimization in Banach spaces\n",
    "- optimization with (partial) differential equations\n",
    "- inverse modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
